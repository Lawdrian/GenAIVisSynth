{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11 Part 1: Self-Attention\n",
    "**Summer Semester 2024**\n",
    "\n",
    "**Author**: Stefan Baumann (stefan.baumann@lmu.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement Self-Attention\n",
    "In this exercise, you will implement multi-head self-attention for a 2D sequence of tokens (shape `B D H W`) yourself using **only basic functions (no pre-made attention implementations!)**. You're allowed to use simple functions such as, e.g., `torch.bmm()`, `torch.nn.functional.softmax()`, ... and simple modules such as `torch.nn.Linear`.\n",
    "\n",
    "Usage of functions provided by the `einops` library (such as `einops.rearrange()`) is also allowed and encouraged (but completely optional!), as it allows writing the code in a nice and concise way by specifying operations across axes of tensors as strings instead of relying on dimension indices.<br>\n",
    "A short introduction into einops is available at https://nbviewer.org/github/arogozhnikov/einops/blob/master/docs/1-einops-basics.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device \"cuda\".\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optional\n",
    "import einops\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device \"{device}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 256,\n",
    "        head_dim: int = 32,\n",
    "        value_dim: int = 32,\n",
    "        num_heads: int = 8,\n",
    "    ):\n",
    "        \"\"\"Multi-Head Self-Attention Module with 2d token input & output\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int, optional): Dimension of the tokens at the input & output. Defaults to 256.\n",
    "            head_dim (int, optional): Per-head dimension of query & key. Defaults to 32.\n",
    "            value_dim (int, optional): Per-head dimension of values. Defaults to 32.\n",
    "            num_heads (int, optional): Number of attention heads. Defaults to 6.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Student.\n",
    "        # Hint: use a single linear layer for q/k/v/out each, and name the respective layers q, k, v, out for the unit tests below to work.\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = head_dim\n",
    "        self.value_dim = value_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        #assert(self.head_dim * num_heads == embed_dim), \"Embed size needs to be divided by heads\"\n",
    "        \n",
    "        \n",
    "        self.q = nn.Linear(embed_dim, head_dim * num_heads, bias=False)\n",
    "        self.k = nn.Linear(embed_dim, head_dim * num_heads, bias=False)\n",
    "        self.v = nn.Linear(embed_dim, value_dim * num_heads, bias=False)\n",
    "        self.out = nn.Linear(value_dim * num_heads, embed_dim, bias=False)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward of multi-head self-attention\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, D, H, W) (batch, embedding dimension, height, width)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (B, D, H, W) (batch, embedding dimension, height, width)\n",
    "        \"\"\"\n",
    "        B, D, H, W = x.shape\n",
    "\n",
    "        # TODO: Student. Don't forget to implement scaling of the attention logits by 1/sqrt(head_dim).\n",
    "        # Implement a standard multi-head self-attention mechanism in a fully batched manner (no explicit for loops etc, pure PyTorch/einops code)\n",
    "        # The expected behavior of this method is that described in Eq. 2 of Attention Is All You Need, Vaswani et al., 2017, NeurIPS.\n",
    "        # In the case of single-head attention, the expected behavior is described by Eq. 1 of the same paper\n",
    "        # Hint when you run into problems:\n",
    "        # For consistency with the multi-head reference implementation the unit test compares against, make sure that the individual heads are arranged correctly in q, k, v, and out.\n",
    "        # The convention is that each head's part in q/k/v is contiguous, i.e., \n",
    "        # if you want to get the query for head 0, it's at q[..., :head_dim], head 1 is at q[..., head_dim:2*head_dim], etc.\n",
    "        \n",
    "        # Flatten height and width to make x suitable for input layers\n",
    "        x = einops.rearrange(x, 'b d h w -> b (h w) d')\n",
    "\n",
    "        # Perform linear transformations\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        # Split q, k, v into multiple heads\n",
    "        q = einops.rearrange(q, 'b q_len (num_heads head_dim)  -> b q_len num_heads head_dim ', num_heads=self.num_heads)\n",
    "        k = einops.rearrange(k, 'b k_len (num_heads head_dim)  -> b k_len num_heads head_dim ', num_heads=self.num_heads)\n",
    "        v = einops.rearrange(v, 'b v_len (num_heads value_dim)  -> b v_len num_heads value_dim ', num_heads=self.num_heads)\n",
    "\n",
    "        # Calculate the attention scores\n",
    "\n",
    "        # q shape: (b, q_len, num_heads, head_dim)\n",
    "        # k shape: (b, k_len, num_heads, head_dim)\n",
    "        energy = torch.einsum(\"bqhd,bkhd -> bhqk\", [q, k])\n",
    "        # energy shape: (b, num_heads, q_len, k_len)\n",
    "\n",
    "        attention = torch.softmax(energy / (self.head_dim ** (0.5)) , dim=3) # apply to key dimension => normalize the scores to sum up 1 one across the source dimension key\n",
    "\n",
    "        # attention shape: (b, num_heads, q_len, k_len)\n",
    "        # v shape: (b, v_len, num_heads, heads_dim)\n",
    "        out = torch.einsum('bhql,blhd -> bqhd', [attention, v]) # multiply k_len and v_len dimensions together\n",
    "        # out shape: (b, q_len, num_heads, head_dim)\n",
    "        \n",
    "        # concatenate num_heads head_dim into 1 dimension\n",
    "        out = einops.rearrange(out, 'b q_len num_heads head_dim -> b q_len (num_heads head_dim)')\n",
    "        # out shape: (b, q_len, (num_heads head_dim) )\n",
    "        \n",
    "        out = self.out(out)\n",
    "        # out shape: (b, q_len, (num_heads head_dim) )\n",
    "\n",
    "        out = einops.rearrange(out, 'b (h w) d -> b d h w', h=H, w=W)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Unit Test (single-head) DO NOT CHANGE!\n",
    "with torch.no_grad():\n",
    "    layer = SelfAttention2d(embed_dim=256, head_dim=256, value_dim=256, num_heads=1).to(device)\n",
    "    x = torch.randn((4, 256, 24, 24), device=device)\n",
    "    res_layer = layer(x)\n",
    "\n",
    "    layer_ref = nn.MultiheadAttention(layer.embed_dim, layer.num_heads).to(device)\n",
    "    layer_ref.load_state_dict({ 'in_proj_weight': torch.cat([layer.q.weight, layer.k.weight, layer.v.weight]), 'out_proj.weight': layer.out.weight }, strict=False)\n",
    "    res_ref = layer_ref(*[x.view(*x.shape[:2], -1).permute(2, 0, 1)] * 3)[0].permute(1, 2, 0).view(*x.shape)\n",
    "    assert torch.allclose(res_layer, res_ref, rtol=1e-2, atol=1e-5), 'Single-head attention result incorrect.'\n",
    "\n",
    "# Unit Test (multi-head) DO NOT CHANGE!\n",
    "with torch.no_grad():\n",
    "    layer = SelfAttention2d().to(device)\n",
    "    x = torch.randn((4, 256, 24, 24), device=device)\n",
    "    res_layer = layer(x)\n",
    "\n",
    "    layer_ref = nn.MultiheadAttention(layer.embed_dim, layer.num_heads).to(device)\n",
    "    layer_ref.load_state_dict({ 'in_proj_weight': torch.cat([layer.q.weight, layer.k.weight, layer.v.weight]), 'out_proj.weight': layer.out.weight }, strict=False)\n",
    "    res_ref = layer_ref(*[x.view(*x.shape[:2], -1).permute(2, 0, 1)] * 3)[0].permute(1, 2, 0).view(*x.shape)\n",
    "    assert torch.allclose(res_layer, res_ref, rtol=1e-2, atol=1e-5), 'Multi-head attention result incorrect.'\n",
    "\n",
    "print('All tests passed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
